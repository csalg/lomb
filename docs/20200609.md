
1. Modeling the Domain

The domain is the shared language between the experts in some area of knowledge or human activity and the developers. This shared language is used to create models of real life processes in software.

Our first goal should be to have a basic idea of our domain, and a basic framework which can then be expanded. This is best done by developing the domain layer and minimising the complexity of the application and infrastructure layers. Our ultimate goal is a reasonably deep and comprehensive model which can be expanded.

What is the domain that we are dealing with?
Linguistics, language learning, NLP.

What are some influential papers in the field and what language is used to define concepts?
- Duolingo paper.
- Nation paper.

Some important concepts:
- Words, word families, roots, lemmas.

Word is a concept which does not have a clear definition in linguistics. Is "run" and "runs" the same? word? For this reason in linguistics there is usually no reference to 'words' but rather _word families_, _roots_, _lemmas_ and _tokens_. 

Following our previous example, "run" and "runs" would belong to the same word family ("run"), but they are different tokens. These tokens, however, also share the same root ("run") and the same lemma ("run").

There are different algorithms to find out the root, part-of-speech and lemma of a token. I am not aware of any mature libraries for grouping words into word families.

Anyway, let us now move on to previous work.

There is surprisingly little work done on how people acquire new vocabulary through reading. Nation is an often quoted paper, in which the authors analysed a corpus of text and came to the conclusion that one needs to know about 9,000 word families in order to have a native-like command of the English language.

The details of this paper are actually very interesting. The authors worked on some previous research that suggested that one needs 8 exposures to a word in order to learn it (which, in the context of that paper was defined as a probability of recall over 50% after 3 weeks). Based on that assumption and some very basic statistics they came to the conclusion that one needs to read around 92 books in order to reach the desired 8 exposures in 9,000 words, covering most of the 1,000 word levels.

Another thing that is interesting about this paper is the relatively small scale of the corpus they used: only 25 books from the public domain.

The Duolingo paper was also very interesting because the model that Duolingo uses is incredibly simple. They just use a generalized linear model. They model the probability of recall following this function:

and then use gradient descent to find optimal weights.

Another interesting paper/concept is that of DKT. DKT are used to create learning itineraries based on user responses to different computer challenges. These challenges are typically presented to the networks as long, sparse vectors, and the machines are basically doing sequence learning on these vectors (using RNNs or LSTM). It is not known how these networks actually work.

2. Interesting problems

- The idea that one only needs to read a word 8 times in order to learn it is at odds with the forgetting curve model. It suggests that it mostly is not too important exactly when a word exposure happens as long as it happens a certain number of times. The authors of the paper admit that spacing plays a role but it isn't known how important of a role, whether seeing a word in context matters or not... But these are all questions which are trivial to answer with some data. 

- The Duolingo model is probably very easy to beat with RNNs or LSTMs.

- Book itineraries. A user wants to read a book. But that book is too difficult. Can we recommend some books that the user will enjoy reading that will gradually prepare the user to read that more difficult book? Similarly, a user wants to do well in a certain exam, or learn the vocabulary that is used in a certain field...

- Cold start problem. A user is new to the platform and reads a page from a book. In doing so he can interact with the book, look definitions up, look translations of sentences... Can we use this very limited dataset on which words have been looked up and which words haven't been looked up to make a predicition on which other words he also knows and which words he doesn't?

3. Building the right domain.

The content problem.

A platform in which users read so that they can improve their reading skills naturally needs books to read. Or texts, tv series... But the problem is these books are copyrighted. 

Additionally, there is some cost in translating a sentence and in lemmatizing the sentence's tokens. If it is done in the client, then the client needs to be a powerful computer with a lot of RAM, because the tagger loads a large NN to memory. If it is done in the server it will make the application incredibly slow, and it will likely be very difficult to provide QoS when lots of users are online at the same time.

The only thing that makes sense is to perform the expensive translation and lemmatization operations once, and then reuse those computations (trade memory for computation). And the logical way to do this is... to modify the ebooks themselves so that they carry around for each sentence a token-to-lemma dictionary and the translated version of the sentence.

In this way books could then be shared or sold with all of this information embedded in them, but the platform would have nothing to do with the expensive and possibly illegal translation / tagging operations.

4. What has been done?

Services that are done:
- Tagger
- Translation
- Library (and reader)
- Tracking
- Revision

There is now a basic single user app with some services. Initially I hadn't considered modifying ebooks themselves, but it is actually not too difficult to do basic modifications of epub files to include the additional data. These epub files could potentially be read in low-powered offline devices like a normal ebook reader (with special software). The modifications are a bit of a mess because it requires parsing a whole XML file and finding the sentences buried in those tags, so my algorithm is very basic and would not work for very complex ebooks. But it will probably work for most ebooks. 

A reader for just text works fine. The tracking infrastructure is also working, and I have a dataset of my own reading progress. I have also implemented a basic algorithm which I use for revision.

The immediate goal is to refactor the tagger and translation services so that they can successfully produce augmented ebooks.

Then I need to expand the current reader (which only works with basic text files) to work with epub files.

Lastly, I need to add authentification and put some work into the frontend so it looks nicer. Then I will get some friends to use it and make modifications following their experiences and user stories. Hopefully after that we can invite additional users to the platform and use their data to work on at least the first two problems outlined.
